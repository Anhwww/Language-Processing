{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Word Semantics and Embeddings\n",
        "\n"
      ],
      "metadata": {
        "id": "R7Tz61-PCU78"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1\n",
        "\n",
        "Download [word vectors](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing) that are pretrained on Google News dataset (approx. 100 billion words). The file contains word vectors of 3 million words/phrases, whose dimentionalities are 300. Print out the word vector of the term “United States”. Note that “United States” is represented as “United_States” in the file."
      ],
      "metadata": {
        "id": "msn-BAl6oZvn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!wget http://download.tensorflow.org/data/word2vec/googlenews-vectors-negative300.bin.gz"
      ],
      "metadata": {
        "id": "cDAw1D6Xp1YC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d90d096-b3ad-4945-e574-8468a8c3ad4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-03-15 16:10:04--  http://download.tensorflow.org/data/word2vec/googlenews-vectors-negative300.bin.gz\n",
            "Resolving download.tensorflow.org (download.tensorflow.org)... 142.251.8.207, 173.194.174.207, 108.177.125.207, ...\n",
            "Connecting to download.tensorflow.org (download.tensorflow.org)|142.251.8.207|:80... connected.\n",
            "HTTP request sent, awaiting response... 403 Forbidden\n",
            "2024-03-15 16:10:04 ERROR 403: Forbidden.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u34gSUW-gjF0",
        "outputId": "6b12c7ff-43a8-4e51-832f-76916580bbe4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "model = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/GoogleNews-vectors-negative300.bin.gz\", binary=True)\n",
        "\n",
        "# Print out the word vector of the term \"United States\"\n",
        "print(model[\"United_States\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQ4LCSdSgFls",
        "outputId": "4c9f776b-3e47-4db2-ba2e-a72894af179a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n",
            "[-3.61328125e-02 -4.83398438e-02  2.35351562e-01  1.74804688e-01\n",
            " -1.46484375e-01 -7.42187500e-02 -1.01562500e-01 -7.71484375e-02\n",
            "  1.09375000e-01 -5.71289062e-02 -1.48437500e-01 -6.00585938e-02\n",
            "  1.74804688e-01 -7.71484375e-02  2.58789062e-02 -7.66601562e-02\n",
            " -3.80859375e-02  1.35742188e-01  3.75976562e-02 -4.19921875e-02\n",
            " -3.56445312e-02  5.34667969e-02  3.68118286e-04 -1.66992188e-01\n",
            " -1.17187500e-01  1.41601562e-01 -1.69921875e-01 -6.49414062e-02\n",
            " -1.66992188e-01  1.00585938e-01  1.15722656e-01 -2.18750000e-01\n",
            " -9.86328125e-02 -2.56347656e-02  1.23046875e-01 -3.54003906e-02\n",
            " -1.58203125e-01 -1.60156250e-01  2.94189453e-02  8.15429688e-02\n",
            "  6.88476562e-02  1.87500000e-01  6.49414062e-02  1.15234375e-01\n",
            " -2.27050781e-02  3.32031250e-01 -3.27148438e-02  1.77734375e-01\n",
            " -2.08007812e-01  4.54101562e-02 -1.23901367e-02  1.19628906e-01\n",
            "  7.44628906e-03 -9.03320312e-03  1.14257812e-01  1.69921875e-01\n",
            " -2.38281250e-01 -2.79541016e-02 -1.21093750e-01  2.47802734e-02\n",
            "  7.71484375e-02 -2.81982422e-02 -4.71191406e-02  1.78222656e-02\n",
            " -1.23046875e-01 -5.32226562e-02  2.68554688e-02 -3.11279297e-02\n",
            " -5.59082031e-02 -5.00488281e-02 -3.73535156e-02  1.25976562e-01\n",
            "  5.61523438e-02  1.51367188e-01  4.29687500e-02 -2.08007812e-01\n",
            " -4.78515625e-02  2.78320312e-02  1.81640625e-01  2.20703125e-01\n",
            " -3.61328125e-02 -8.39843750e-02 -3.69548798e-05 -9.52148438e-02\n",
            " -1.25000000e-01 -1.95312500e-01 -1.50390625e-01 -4.15039062e-02\n",
            "  1.31835938e-01  1.17675781e-01  1.91650391e-02  5.51757812e-02\n",
            " -9.42382812e-02 -1.08886719e-01  7.32421875e-02 -1.15234375e-01\n",
            "  8.93554688e-02 -1.40625000e-01  1.45507812e-01  4.49218750e-02\n",
            " -1.10473633e-02 -1.62353516e-02  4.05883789e-03  3.75976562e-02\n",
            " -6.98242188e-02 -5.46875000e-02  2.17285156e-02 -9.47265625e-02\n",
            "  4.24804688e-02  1.81884766e-02 -1.73339844e-02  4.63867188e-02\n",
            " -1.42578125e-01  1.99218750e-01  1.10839844e-01  2.58789062e-02\n",
            " -7.08007812e-02 -5.54199219e-02  3.45703125e-01  1.61132812e-01\n",
            " -2.44140625e-01 -2.59765625e-01 -9.71679688e-02  8.00781250e-02\n",
            " -8.78906250e-02 -7.22656250e-02  1.42578125e-01 -8.54492188e-02\n",
            " -3.18359375e-01  8.30078125e-02  6.34765625e-02  1.64062500e-01\n",
            " -1.92382812e-01 -1.17675781e-01 -5.41992188e-02 -1.56250000e-01\n",
            " -1.21582031e-01 -4.95605469e-02  1.20117188e-01 -3.83300781e-02\n",
            "  5.51757812e-02 -8.97216797e-03  4.32128906e-02  6.93359375e-02\n",
            "  8.93554688e-02  2.53906250e-01  1.65039062e-01  1.64062500e-01\n",
            " -1.41601562e-01  4.58984375e-02  1.97265625e-01 -8.98437500e-02\n",
            "  3.90625000e-02 -1.51367188e-01 -8.60595703e-03 -1.17675781e-01\n",
            " -1.97265625e-01 -1.12792969e-01  1.29882812e-01  1.96289062e-01\n",
            "  1.56402588e-03  3.93066406e-02  2.17773438e-01 -1.43554688e-01\n",
            "  6.03027344e-02 -1.35742188e-01  1.16210938e-01 -1.59912109e-02\n",
            "  2.79296875e-01  1.46484375e-01 -1.19628906e-01  1.76757812e-01\n",
            "  1.28906250e-01 -1.49414062e-01  6.93359375e-02 -1.72851562e-01\n",
            "  9.22851562e-02  1.33056641e-02 -2.00195312e-01 -9.76562500e-02\n",
            " -1.65039062e-01 -2.46093750e-01 -2.35595703e-02 -2.11914062e-01\n",
            "  1.84570312e-01 -1.85546875e-02  2.16796875e-01  5.05371094e-02\n",
            "  2.02636719e-02  4.25781250e-01  1.28906250e-01 -2.77099609e-02\n",
            "  1.29882812e-01 -1.15722656e-01 -2.05078125e-02  1.49414062e-01\n",
            "  7.81250000e-03 -2.05078125e-01 -8.05664062e-02 -2.67578125e-01\n",
            " -2.29492188e-02 -8.20312500e-02  8.64257812e-02  7.61718750e-02\n",
            " -3.66210938e-02  5.22460938e-02 -1.22070312e-01 -1.44042969e-02\n",
            " -2.69531250e-01  8.44726562e-02 -2.52685547e-02 -2.96630859e-02\n",
            " -1.68945312e-01  1.93359375e-01 -1.08398438e-01  1.94091797e-02\n",
            " -1.80664062e-01  1.93359375e-01 -7.08007812e-02  5.85937500e-02\n",
            " -1.01562500e-01 -1.31835938e-01  7.51953125e-02 -7.66601562e-02\n",
            "  3.37219238e-03 -8.59375000e-02  1.25000000e-01  2.92968750e-02\n",
            "  1.70898438e-01 -9.37500000e-02 -1.09375000e-01 -2.50244141e-02\n",
            "  2.11914062e-01 -4.44335938e-02  6.12792969e-02  2.62451172e-02\n",
            " -1.77734375e-01  1.23046875e-01 -7.42187500e-02 -1.67968750e-01\n",
            " -1.08886719e-01 -9.04083252e-04 -7.37304688e-02  5.49316406e-02\n",
            "  6.03027344e-02  8.39843750e-02  9.17968750e-02 -1.32812500e-01\n",
            "  1.22070312e-01 -8.78906250e-03  1.19140625e-01 -1.94335938e-01\n",
            " -6.64062500e-02 -2.07031250e-01  7.37304688e-02  8.93554688e-02\n",
            "  1.81884766e-02 -1.20605469e-01 -2.61230469e-02  2.67333984e-02\n",
            "  7.76367188e-02 -8.30078125e-02  6.78710938e-02 -3.54003906e-02\n",
            "  3.10546875e-01 -2.42919922e-02 -1.41601562e-01 -2.08007812e-01\n",
            " -4.57763672e-03 -6.54296875e-02 -4.95605469e-02  2.22656250e-01\n",
            "  1.53320312e-01 -1.38671875e-01 -5.24902344e-02  4.24804688e-02\n",
            " -2.38281250e-01  1.56250000e-01  5.83648682e-04 -1.20605469e-01\n",
            " -9.22851562e-02 -4.44335938e-02  3.61328125e-02 -1.86767578e-02\n",
            " -8.25195312e-02 -8.25195312e-02 -4.05273438e-02  1.19018555e-02\n",
            "  1.69921875e-01 -2.80761719e-02  3.03649902e-03  9.32617188e-02\n",
            " -8.49609375e-02  1.57470703e-02  7.03125000e-02  1.62353516e-02\n",
            " -2.27050781e-02  3.51562500e-02  2.47070312e-01 -2.67333984e-02]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2\n",
        "\n",
        "Compute the cosine similarity between “United States” and “U.S.”"
      ],
      "metadata": {
        "id": "lm31XvAfrlrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write your code here\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Get the word vectors for \"United States\" and \"U.S.\"\n",
        "vector_us = model[\"United_States\"]\n",
        "vector_us_abbr = model[\"U.S.\"]\n",
        "\n",
        "# Compute the cosine similarity\n",
        "cos_sim = np.dot(vector_us, vector_us_abbr) / (np.linalg.norm(vector_us) * np.linalg.norm(vector_us_abbr))\n",
        "\n",
        "print(\"Cosine similarity: {:.4f}\".format(cos_sim))"
      ],
      "metadata": {
        "id": "gQDoB9p0rtYE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d262923f-680a-4f7e-9ec7-8b319df234fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity: 0.7311\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3\n",
        "Find the top-10 words that have the highest cosine similarity with the word “United States” and print out the similarity score."
      ],
      "metadata": {
        "id": "LopFssN_rwcV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write your code here\n",
        "# Find the top-10 words that have the highest cosine similarity with \"United States\"\n",
        "top_10 = model.most_similar(\"United_States\", topn=10)\n",
        "\n",
        "# Print out the similarity score\n",
        "for word, similarity in top_10:\n",
        "    print(\"{}: {:.4f}\".format(word, similarity))"
      ],
      "metadata": {
        "id": "7sYGLaSkr1yf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7e6b03ce-5a5a-42d1-ea28-a50c9f11366d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unites_States: 0.7877\n",
            "Untied_States: 0.7541\n",
            "United_Sates: 0.7401\n",
            "U.S.: 0.7311\n",
            "theUnited_States: 0.6404\n",
            "America: 0.6178\n",
            "UnitedStates: 0.6167\n",
            "Europe: 0.6133\n",
            "countries: 0.6045\n",
            "Canada: 0.6019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4\n",
        "\n",
        "Subtract the vector of “Madrid” from the vector of “Spain” and then add the vector of “Athens”. Compute the top-10 most similar words with the output vector."
      ],
      "metadata": {
        "id": "tS8fsHOrr5AX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TODO: Write your code here\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# Get the word vectors for \"Madrid\", \"Spain\", and \"Athens\"\n",
        "vector_madrid = model[\"Madrid\"]\n",
        "vector_spain = model[\"Spain\"]\n",
        "vector_athens = model[\"Athens\"]\n",
        "\n",
        "# Subtract the vector of \"Madrid\" from the vector of \"Spain\" and then add the vector of \"Athens\"\n",
        "output_vector = vector_spain - vector_madrid + vector_athens\n",
        "\n",
        "# Find the top-10 most similar words with the output vector\n",
        "top_10 = model.most_similar(positive=[output_vector], topn=10)\n",
        "\n",
        "# Print out the similarity score\n",
        "for word, similarity in top_10:\n",
        "    print(\"{}: {:.4f}\".format(word, similarity))"
      ],
      "metadata": {
        "id": "4d143lrfsBZv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99beff7f-9b85-4c0d-f06c-97ddcbdda551"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Athens: 0.7528\n",
            "Greece: 0.6685\n",
            "Aristeidis_Grigoriadis: 0.5496\n",
            "Ioannis_Drymonakos: 0.5361\n",
            "Greeks: 0.5352\n",
            "Ioannis_Christou: 0.5330\n",
            "Hrysopiyi_Devetzi: 0.5088\n",
            "Iraklion: 0.5059\n",
            "Greek: 0.5041\n",
            "Athens_Greece: 0.5034\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 5\n",
        "Download [word analogy evaluation dataset](http://download.tensorflow.org/data/questions-words.txt). Compute the vector as follows: vec(word in second column) - vec(word in first column) + vec(word in third column). From the output vector, find the most similar word. Append the most similar word and its similarity to each row of the downloaded file."
      ],
      "metadata": {
        "id": "gkZP38QWsEJX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import requests\n",
        "\n",
        "# Download the word analogy evaluation dataset\n",
        "url = \"http://download.tensorflow.org/data/questions-words.txt\"\n",
        "data = requests.get(url).text.split(\"\\n\")[:1000]\n",
        "\n",
        "correct = 0\n",
        "total = 0\n",
        "# Process each row in the dataset\n",
        "for line in data:\n",
        "    words = line.strip().split()\n",
        "    if len(words) == 4:\n",
        "        word1, word2, word3, word4 = words\n",
        "        if all(word in model for word in [word1, word2, word3]):\n",
        "            # Compute analogy vector\n",
        "            vector = model[word2] - model[word1] + model[word3]\n",
        "            # Find most similar word\n",
        "            most_similar_word, similarity = model.similar_by_vector(vector, topn=1)[0]\n",
        "            # Append results to the row\n",
        "            line += f\"\\t{most_similar_word}\\t{similarity}\\n\"\n",
        "            # Check if the most similar word is the same as the word in the 4th column\n",
        "            if most_similar_word == word4:\n",
        "                correct += 1\n",
        "            # Increment the total counter\n",
        "            total += 1\n",
        "        else:\n",
        "            line += \"\\tNA\\tNA\\n\"  # If any of the words are not in the vocabulary\n",
        "    else:\n",
        "        line += \"\\tNA\\tNA\\n\"  # If the row is not a valid analogy question\n",
        "\n",
        "    # Write the updated row to the output file\n",
        "    with open('output_file.txt', 'a') as output_file:\n",
        "        output_file.write(line)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "0aqYLkGhsYB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6 (Bonus points)\n",
        "\n",
        "From the output of the exercise 5, compute the accuracy score. It means that you will calculate the percentage of cases in which the most similar words returned by your code are the same as the words in 4th column.\n",
        "\n"
      ],
      "metadata": {
        "id": "a1hShhvdsaxo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#I added a part of exercise 6 in exercise 5.\n",
        "# Compute the accuracy score\n",
        "accuracy = correct / total * 100\n",
        "print(f\"Accuracy score: {accuracy}%\")"
      ],
      "metadata": {
        "id": "G7UYdv-Nsdh-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eeaecd69-3b50-4423-d3e0-356bd1206957"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy score: 30.160320641282567%\n"
          ]
        }
      ]
    }
  ]
}